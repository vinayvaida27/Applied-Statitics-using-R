---
title: "Time Series Lab and assignment"
author: "Vinay Vaida"
date: "2023-11-02"
output:
  word_document: default
  pdf_document: default
---


# TimeSeries Lab & Assignment

Dataset: "birth" from libraty astsa, U.S. Monthly Live Births 1950-1980

```{r}
library(astsa)
data(birth)

```



```{r}
#plot(birth)
library(ggplot2)
library(ggfortify)
library(dplyr)
birth %>%
  autoplot() + ggtitle("U.S. Monthly Live Births 1950-1980")
```

A seasonal plot is similar to a time plot except that the data are plotted against the individual “seasons” in which the data were observed. 

```{r}
library(forecast)
ggseasonplot(birth, year.labels=TRUE, year.labels.left=TRUE) +
  ylab(" ylabel") +
  ggtitle("Seasonal plot: U.S. Monthly Live Births 1950-1980")
```



We are going to try a few things to get a feeling about the cyclical 
nature of the dataset. 

There seems to be a yearly cycle. We can try adding  monthly variables or use a sin and/or cosing with the right frequency for a year repetition.

Note: I added numbers to the names of the month because otherwise r will order them alphabetically.

```{r}
n<-length(birth)
#n=373, n/12 = 31.08
month<-rep(c("01Jan","02Feb","03Mar","04Apr","05May","06Jun","07Jul","08Aug","09Sep","10Oct","11Nov","12Dec"),32)[1:n]
times<-1:n

# we won't use all the monthly dummy variables because Jan = when all other are 0
#X<-as.data.frame(cbind(times,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec))
X<-data.frame(times=times,month=month)

# alternatively, seasons can be created with sin, cos
sint=sin(2*pi*times/12)
cost=cos(2*pi*times/12)
X_jan=data.frame(times=times,sint=sint,cost=cost,Jan=rep(c(1,0,0,0,0,0,0,0,0,0,0,0),32)[1:n])  

```

Let's look at the auto correlation function and partial auto correlation functions
```{r}
#acf(birth)
#pacf(birth)
birth %>% ggtsdisplay(main="U.S. Monthly Live Births 1950-1980")
```

Ussing ggplot2

```{r}
ggAcf(birth,lag=48) # default is lag=24
```






```{r}
#acf(diff(birth,1))
birth %>% diff() %>% ggtsdisplay(main="Differences of Birth series")
```


Let's fit a model with monthly dummy variables. There is a curve trend that is beyond quadratic.
```{r}
lsfit=lm(birth~poly(times,3)+month,
 #          Feb+Mar+Apr+May+Jun+Jul+Aug+Sep+Oct+Nov+Dec,
         data=X) 
summary(lsfit)

acf(lsfit$res)
```
Although this looks like a good fit, we see that the residuals have autocorrelation.


Let's also fit a model with sin and cos to model cyclical nature.
```{r}
lsfit_jan=lm(birth~poly(times,3)+sint+cost+Jan,data=X_jan) #you remove sin/cos and do all months
summary(lsfit_jan)
acf(lsfit_jan$res)
```
Same problem with this model, we also see that the residuals still have autocorrelation.



Let's plot both models:
```{r}
plot(times,birth,type="l",main="U.S. Monthly Live Births 1950-1980",col=4) 

lines(times,lsfit$fitted.values,col=2) 
lines(times,lsfit_jan$fitted,col=3) 
legend(329,405,c("birth","lsfit","lsfit_jan"),col=c(4,2,3),lty=1,cex=.5)

#df<-data.frame(fit=lsfit$fitted.values, times=times)
#df2<-data.frame(fit=lsfit_jan$fitted.values, times=times)
#birth %>%
#  autoplot(,col="darkgrey") + 
#  ggtitle("U.S. Monthly Live Births 1950-1980") +
#  geom_line(data=df,aes(x=time(birth),y=fit),col=2)+
#  geom_line(data=df2,aes(x=time(birth),y=fit),col=3)

```

Which model performs better?
```{r}
aic<-round(c(AIC(lsfit), AIC(lsfit_jan)),2)
bic<-round(c(BIC(lsfit), BIC(lsfit_jan)),2)
adjr2<-round(c(summary(lsfit)$ad,summary(lsfit_jan)$ad),2)
rbind(c("lsfit", "lsfit_jan"), aic,bic,adjr2)
```

Now let's try the time series model with auto-regressive, integrated, moving averages and cyclic components:
```{r}
library(forecast)
birthmod<-auto.arima(birth)
birthmod
```

The result is ARIMA(0,1,2)(1,1,1)[12] 
We also see the aic and the bic metrics and this model performed better that the ones we did earlier.



Equation corresponding to the time series model:    
\[(I- sar1 B^12)(I-B^12)(I-B)y_t = (I+sma1 B^12) (I+ma1 B + ma2 B^2) w_t \]
where $\{w_t\}$ are the random errors.

Plugging in the numbers:
\[(I- 0.1018 B^12)(I-B^{12})(I-B)y_t = (I-0.8434 B^{12}) (I-0.3984 B -0.1632 B^2) w_t \]

Or
\[(I-B^{12})(I-B)y_t - 0.1018 B^{12} (I-B^{12})(I-B)y_t 
=  (I-0.3984 B -0.1632 B^2) w_t - 0.8434 B^{12} (I-0.3984 B -0.1632 B^2) w_t \]

\[(I-B^{12})(y_t -y_{t-1})- 0.1018 B^{12}
(I-B^{12})(y_t-y_{t-1}) =  
(w_t-0.3984 w_{t-1} -0.1632 w_{t-2}) - 0.8434  (w_{t-12}-0.3984 w_{t-13} -0.1632 w_{t-14}) \]

\[(y_t -y_{t-1})- (y_{t-12} -y_{t-13})
- 0.1018 B^{12}
((y_t-y_{t-1}) -(y_{t-12}-y_{t-13})) =  
(w_t-0.3984 w_{t-1} -0.1632 w_{t-2}) - 0.8434  (w_{t-12}-0.3984 w_{t-13} -0.1632 w_{t-14}) \]

\[(y_t -y_{t-1})- (y_{t-12} -y_{t-13})
- 0.1018 ((y_{t-12}-y_{t-13}) -(y_{t-24}-y_{t-25})) =  
w_t-0.3984 w_{t-1} -0.1632 w_{t-2}- 0.8434 w_{t-12} + 0.8434*0.3984 w_{t-13} +0.8434*0.1632 w_{t-14} \]

\[(y_t = y_{t-1} + (y_{t-12} -y_{t-13})
+ 0.1018 ((y_{t-12}-y_{t-13}) -(y_{t-24}-y_{t-25})) + 
w_t-0.3984 w_{t-1} -0.1632 w_{t-2}- 0.8434 w_{t-12} + 0.8434*0.3984 w_{t-13} +0.8434*0.1632 w_{t-14} \]
We see that this is quite a complicated structure that captures a yearly cycle plus a 2 year cycle. That seems to account for the curved patterns we observed in the plot of the values.

Let's see the decomposition of the cycles:
```{r}
birth %>% decompose() %>%
  autoplot() + xlab("Year") +
  ggtitle("birth")
#dbirth<-decompose(birth)
#plot(dbirth)
```

We see the trend (2nd plot), the seasonal component (3rd plot) and the random part (4th plot). The 1st plot is the original series.


* Trend: the trend-cycle component $T_t$ is a m-moving average, where m is the cycle. In our case of montly data, $m= 12$. (moving average = average of previous m-observations)


* Detrended series: Calculate the detrended series as  $y_t-T_t$

* Seasonal component: the seasonal component for each season is the average of the detrended values for that season. This gives a series called $S_t$.

* Error: The remainder component is calculated by subtracting the estimated seasonal and trend-cycle components: $R_t=Y_t-T_t-S_t$.








Let's plot the fitted values of the 3 models:
```{r}
plot(times,birth,type="l") #plot on original scale
#lines(times,birth) #add lines to existing plot
lines(times,lsfit$fitted.values,col=2) #undo log for fitted model
lines(times,lsfit_jan$fitted,col=3) #undo log for fitted model
lines(times,birthmod$fitted,col=4)
legend(329,405,c("lsfit","lsfit_jan","arima"),col=c(2,3,4),lty=1,cex=.5)
```



Now let's use our arima model to do forecasts:
```{r}
plot(forecast(birthmod, 24), xlab ="Monthly Data",
     ylab ="N. Births",
     main ="Number of Birth per month", col.main ="darkgreen")
```


Let's check that the errors do not have any auto-correlation:
```{r}
acf(birthmod$residuals)
```



Just for the heck of it, let's look at the differences involved in the arima model:

```{r}
plot(diff(birth,1),main="one lag difference",ylab="Diff")
plot(diff( diff(birth,1) ,12),main="one year difference \n of the one-lad differences",ylab="Diff-12 of Diff")
```











# Time Series Assignment
  
We will fit a model to the log of the Australian wine sales. 

* Plot wine and log(wine).

* Plot the auto correlation and partial auto correlation functions for log(wine).

* Just as we did for "birth", fit a model allowing for a term for each month and time. 

* Just as we did for "birth", fit a model using sin and cos to model seasonality and time.

* compute the aic, bic and adjusted $r^2$ corresponding to both models.

* Use auto.arima() to obtain the arima model.

* compare the aic and bic of the arima model to the previous 2 models.

* Write down the equation corresponding to the arima model.

* Plot the decomposition of the series. 

* Plot the fitted values of all 3 models over the values of wine. Remember that your models were for log(wine) but you are plotting wine, so you need to adjust your fitted values.
  
* plot the predicted values for the next 12 months.

* auto.arima does not work with covariates. But we can use the structure it developed to add one or several covarites. 
Consider the models: 
    - Arima(y, order = c(1,1,1), xreg = X)
and
    - Arima(y, order = c(1,0,1), xreg = X)
where X is the data frame with times and the monthly dummy variables


```{r}
wine=c(
.46400E+03,
.67500E+03,
.70300E+03,
.88700E+03,
.11390E+04,
.10770E+04,
.13180E+04,
.12600E+04,
.11200E+04,
.96300E+03,
.99600E+03,
.96000E+03,
.53000E+03,
.88300E+03,
.89400E+03,
.10450E+04,
.11990E+04,
.12870E+04,
.15650E+04,
.15770E+04,
.10760E+04,
.91800E+03,
.10080E+04,
.10630E+04,
.54400E+03,
.63500E+03,
.80400E+03,
.98000E+03,
.10180E+04,
.10640E+04,
.14040E+04,
.12860E+04,
.11040E+04,
.99900E+03,
.99600E+03,
.10150E+04,
.61500E+03,
.72200E+03,
.83200E+03,
.97700E+03,
.12700E+04,
.14370E+04,
.15200E+04,
.17080E+04,
.11510E+04,
.93400E+03,
.11590E+04,
.12090E+04,
.69900E+03,
.83000E+03,
.99600E+03,
.11240E+04,
.14580E+04,
.12700E+04,
.17530E+04,
.22580E+04,
.12080E+04,
.12410E+04,
.12650E+04,
.18280E+04,
.80900E+03,
.99700E+03,
.11640E+04,
.12050E+04,
.15380E+04,
.15130E+04,
.13780E+04,
.20830E+04,
.13570E+04,
.15360E+04,
.15260E+04,
.13760E+04,
.77900E+03,
.10050E+04,
.11930E+04,
.15220E+04,
.15390E+04,
.15460E+04,
.21160E+04,
.23260E+04,
.15960E+04,
.13560E+04,
.15530E+04,
.16130E+04,
.81400E+03,
.11500E+04,
.12250E+04,
.16910E+04,
.17590E+04,
.17540E+04,
.21000E+04,
.20620E+04,
.20120E+04,
.18970E+04,
.19640E+04,
.21860E+04,
.96600E+03,
.15490E+04,
.15380E+04,
.16120E+04,
.20780E+04,
.21370E+04,
.29070E+04,
.22490E+04,
.18830E+04,
.17390E+04,
.18280E+04,
.18680E+04,
.11380E+04,
.14300E+04,
.18090E+04,
.17630E+04,
.22000E+04,
.20670E+04,
.25030E+04,
.21410E+04,
.21030E+04,
.19720E+04,
.21810E+04,
.23440E+04,
.97000E+03,
.11990E+04,
.17180E+04,
.16830E+04,
.20250E+04,
.20510E+04,
.24390E+04,
.23530E+04,
.22300E+04,
.18520E+04,
.21470E+04,
.22860E+04,
.10070E+04,
.16650E+04,
.16420E+04,
.15250E+04,
.18380E+04,
.18920E+04,
.29200E+04,
.25720E+04,
.26170E+04,
.20470E+04)
```



```{r}
y=log(wine)
times=1:142

Jan=rep(c(1,0,0,0,0,0,0,0,0,0,0,0),12)[1:142]
Feb=rep(c(0,1,0,0,0,0,0,0,0,0,0,0),12)[1:142]
Mar=rep(c(0,0,1,0,0,0,0,0,0,0,0,0),12)[1:142]
Apr=rep(c(0,0,0,1,0,0,0,0,0,0,0,0),12)[1:142]
May=rep(c(0,0,0,0,1,0,0,0,0,0,0,0),12)[1:142]
Jun=rep(c(0,0,0,0,0,1,0,0,0,0,0,0),12)[1:142]
Jul=rep(c(0,0,0,0,0,0,1,0,0,0,0,0),12)[1:142]
Aug=rep(c(0,0,0,0,0,0,0,1,0,0,0,0),12)[1:142]
Sep=rep(c(0,0,0,0,0,0,0,0,1,0,0,0),12)[1:142]
Oct=rep(c(0,0,0,0,0,0,0,0,0,1,0,0),12)[1:142]
Nov=rep(c(0,0,0,0,0,0,0,0,0,0,1,0),12)[1:142]
Dec=rep(c(0,0,0,0,0,0,0,0,0,0,0,1),12)[1:142]
sint=sin(2*pi*times/12)
cost=cos(2*pi*times/12)
X=cbind(times,Feb,Mar,Apr,May,Jun,Jul,Aug,Sep,Oct,Nov,Dec)  #sin and cos and constant for Jan; 
X_jan=cbind(times,sint,cost,Jan)  #sin and cos and constant for Jan; 
```

We will fit a model to the log of the Australian wine sales. 

* Plot wine and log(wine).
```{r}
library(ggplot2)

if (!requireNamespace("astsa", quietly = TRUE)) {
  install.packages("astsa")
}

# Loading required libraries
library(astsa)

# Plotting wine and log(wine)
plot(wine, type = "l", col = "green", ylab = "Wine-Sales", xlab = "Time", main = "Australian Wine Sales")
plot(log(wine), type = "l", col = "blue", ylab = "Wine Sales", xlab = "Time", main = "Log Of Australian Wine Sales")
legend("topleft", legend = c("Wine", "Log(Wine)"), col = c("green", "blue"), lty = c(1, 2))


```
* Plot the auto correlation and partial auto correlation functions for log(wine).
```{r}
# Calculating and plotting ACF and PACF for log(wine)
par(mfrow = c(2, 1))
acf(log(wine), main = "ACF of log(Wine)")
pacf(log(wine), main = "PACF of log(Wine)")
```

* Just as we did for "birth", fit a model allowing for a term for each month and time. 
```{r}
# Fitting a model with a term for each month and time
lm_model <- lm(log(wine) ~ poly(times,3)  + Jan + Feb + Mar + Apr + May + Jun + Jul + Aug + Sep + Oct + Nov + Dec, data = data.frame(times,Jan, Feb, Mar, Apr, May, Jun, Jul, Aug, Sep, Oct, Nov, Dec, log(wine)))

# Displaying the summary of the model
summary(lm_model)
```
* Just as we did for "birth", fit a model using sin and cos to model seasonality and time.
```{r}
# Fitting a model with sin and cos for seasonality and time
lm_model_sin_cos <- lm(log(wine) ~ poly(times,3)+sint + cost + Jan, data = data.frame(times, sint, cost, Jan, log(wine)))

# Displaying the summary of the model
summary(lm_model_sin_cos)

```

* compute the aic, bic and adjusted $r^2$ corresponding to both models.

```{r}
# Computing AIC, BIC, and adjusted R^2 for the model with a term for each month and time
lm_aic_bic_adjr2 <- c(AIC(lm_model), BIC(lm_model), summary(lm_model)$adj.r.squared)
lm_aic_bic_adjr2

```
```{r}
# Computing AIC, BIC, and adjusted R^2 for the model with sin and cos for seasonality and time
lm_sin_cos_aic_bic_adjr2 <- c(AIC(lm_model_sin_cos), BIC(lm_model_sin_cos), summary(lm_model_sin_cos)$adj.r.squared)
lm_sin_cos_aic_bic_adjr2

```

* Use auto.arima() to obtain the arima model.

```{r}
library(forecast)

# Using auto.arima to obtain the ARIMA model
arima_model <- auto.arima(log(wine))

# Displaying the obtained ARIMA model
arima_model

```
* compare the aic and bic of the arima model to the previous 2 models.

```{r}
# AIC and BIC of the model with a term for each month and time
lm_aic_bic <- c(AIC(lm_model), BIC(lm_model))

# AIC and BIC of the model with sin and cos for seasonality and time
lm_sin_cos_aic_bic <- c(AIC(lm_model_sin_cos), BIC(lm_model_sin_cos))

# Displaying all AIC and BIC values for comparison
comparison <- data.frame(
  Models = c("lm_model", "lm_model_sin_cos", "arima_model"),
  AIC = c(lm_aic_bic[1], lm_sin_cos_aic_bic[1], arima_model$aic),
  BIC = c(lm_aic_bic[2], lm_sin_cos_aic_bic[2], arima_model$bic)
)
comparison

```
* Write down the equation corresponding to the arima model.

:(1-Ø1B)(1-B)(yt – yt-1)=(1+Ɵ1B)wt
-	Ø1 is the autoregressive parameter, 
-	B is the backshift operator (used for differencing),
-	yt is the observed time series, 
-	yt-1 is the lagged value of the time series, 
-	Ɵ1 is the moving average parameter, 
-	wt is the white noise series.
The estimated values for Ø1 and Ɵ1 are 0.5214 and -0.9277, respectively, based on your model summary. You can substitute these values into the equation to get the specific form for your ARIMA(1,1,1) model.

* Plot the decomposition of the series. 

```{r}
library(forecast)
# Convert the y to a time series (replace 'frequency = 12' with your actual frequency if different)
wine_ts <- ts(wine, frequency = 12)
# Try decomposing the time series
decomposed <- try(decompose(wine_ts))
# Plot the decomposition if successful
if (class(decomposed) != "try-error") {
autoplot(decomposed)
} else {
cat("Unable to decompose the time series.")
}
```
* Plot the fitted values of all 3 models over the values of wine. Remember that your models were for log(wine) but you are plotting wine, so you need to adjust your fitted values.

```{r}
# Creating a dataframe with original and fitted values in the original scale
# Creating a data frame with original and fitted values
df_fitted <- data.frame(
  times = 1:length(wine),
  wine = wine,
  lm_fitted = exp(predict(lm_model)),
  lm_sin_cos_fitted = exp(predict(lm_model_sin_cos)),
  arima_fitted = exp(fitted(arima_model))
)

# Plotting the original wine values and the fitted values from the models
plot(df_fitted$times, df_fitted$wine, type = "l", xlab = "Time", ylab = "Wine Sales",
     main = "Original Wine Sales vs Fitted Values")
lines(df_fitted$times, df_fitted$lm_fitted, col = "red")
lines(df_fitted$times, df_fitted$lm_sin_cos_fitted, col = "blue")
lines(df_fitted$times, df_fitted$arima_fitted, col = "brown")
legend("topleft", legend = c("Original", "LM Fitted", "LM Sin/Cos Fitted", "ARIMA Fitted"),
       fill  = c("black", "red", "blue", "brown"))






```
* plot the predicted values for the next 12 months.

```{r}
# Generating forecasts for the next 12 months
forecast_values <- forecast(arima_model, h = 12)

# Plotting the forecasted values
plot(forecast_values, xlab = "Time", ylab = "Wine Sales",
     main = "Forecasted Wine Sales for the Next 12 Months")

```

* auto.arima does not work with covariates. But we can use the structure it developed to add one or several covarites. 
Consider the models: 
    - Arima(y, order = c(1,1,1), xreg = X)
and
    - Arima(y, order = c(1,0,1), xreg = X)
where X is the data frame with times and the monthly dummy variables

```{r}
# Fitting the ARIMA model with covariates and differencing
arima_model_xreg_diff <- Arima(log(wine), order = c(1, 1, 1), xreg = X)

# Displaying the model summary
summary(arima_model_xreg_diff)

```

```{r}
# Fitting the ARIMA model with covariates without differencing
arima_model_xreg <- Arima(log(wine), order = c(1, 0, 1), xreg = X)

# Displaying the model summary
summary(arima_model_xreg)

```





